---
layout: post
title: "Day 22 â€“ Achieving Higher Rewards"
date: 2025-06-25
author: Anuva Nuzhat
permalink: /day22.html
tags: ["TD1", "RL", "Training", "DQN", "Dueling DQN"]

what_i_learned: >
  Today I began trying to optimize rewards and get my accuracy as high as it could be. At first I wasn't having much luck but I approached the calculate reward
  function differently. Instead of looking at specific ranges and giving penalities and rewards based off of that, I started giving my model
  high rewards for being in the optimal glucose and bp ranges, and then gave rewards or penalties based off the the models
  distance to the target range. I also implemented an if statement tat checks to see if the the model is near target to penalize it
  for making any drastic decisions. I adjusted my hyperparameters too and found a set that I think works well for my model by giving it a slower decay
  and a higher learning rate. By the end of today I started reaching 75~90% accuracy ranges. I then thought about how I wanted to improve it even more
  as 90% was the highest I had gotten.
  

blockers: >
  Implementing OpenAI baselines PER Replay Buffer.
  
reflection: >
  Getting a 90% accuracy was definitely a one time luck thing so I tried thinking of different ways to improve the accuracy.
  I didn't want to change the calculate reward function so I looked into other things I could change. Something that caught my eye was
  a different type of replay memory buffer. I saw tha openAI had posted their PER replay_buffer so I tried incorporating my model with their 
  replay buffer. However I was having difficulties implementing it so I went back to my original
  and looked at the optimize function instead. I decided to go back to a simple optimization function and tried running it again.
  I was able to achieve 100% accuracy! I ran it again just to make sure and got 95%! I'm really happy that after days
  of struggling with this I was able to finally get the results I wanted.
  
---
