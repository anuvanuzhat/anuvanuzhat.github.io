layout: post
title: "Day 21 – Starting Over - Calculating New Rewards"
date: 2025-06-24
author: Anuva Nuzhat
permalink: /day21.html
tags: ["TD1", "RL", "Training", "DQN", "Dueling DQN"]

what_i_learned: >
  Today I spent the entire day trying to optimize my model. I made an accuracy function to track how often my model correctly predicted blood pressure 
  and glucose levels within their respective target ranges over different episode intervals. At first, my accuracy was just 19%, then I managed to raise it to 50%, 
  but that improvement didn’t last. After some adjustments, my accuracy dropped to 0% and remained there, no matter what changes I made. It seems the model 
  understands optimal blood pressure levels but continues to struggle with identifying the most effective glucose range, which causes the overall performance to collapse.

blockers: >
  Figuring out the most effective reward and penalty structure to guide the model's learning remains extremely challenging.

reflection: >
  Failing so many times made it clear that I needed a bigger change. I decided to increase the number of discrete action choices from 9 to 25. 
  That modification led to some progress—glucose levels began hitting target in certain episodes. My new accuracy improved from 0% to 21.5%, 
  which was encouraging! Additionally, the epsilon decay curve improved after I adjusted it from 0.9995 to 0.995 and slightly increased the learning rate. 
  For next steps, I plan to revisit my penalty values and consider increasing the number of training episodes to give the model more time to learn effectively.
