---
layout: post
title: "Day 17 - Training and Dueling DQN"
date: 2025-06-18
author: Anuva Nuzhat
permalink: /day17.html
tags: ["DQN", "RL", "Dueling DQN", "Training"]

what_i_learned: |
  Today in the morning we practiced pitching our models without using key words to be able to explain it to
  ordinary day people. After practicing we pitched it to the high school teacher that we would be meeting today.
  These teachers are here to learn more about AI and hopefully help and teach their students by watching us and 
  learning from our research. 
  
  After, I began to train my DQN model and I tried out a couple different epsilon decays 
  to see how it would improve the models learning. Unfortunetly this took a very look time as each time I changed the 
  epsilon decay rate my model took quite some time to run. At first with a larger epsilon decay rate I noticed that after 
  1000 runs the model did not reach 0 for the epsilon decay. I then began to make my decay rate smaller so the epsilon decay
  would decrease faster. I want the decay to be as close to zero as possible so it gains the most knowledge and makes
  more informed decisions rather than making random ones and finding the optimal policy. After training this normal DQN
  my goal for tomorrow is to work on a Dueling DQN as my teammates are looking into other DQN's so we can compare them and 
  find the best one for testing our data on. 

blockers: |
  The models took quite a long time to run and when experimenting with different epsilon decay rates or learning rates it
  wasn't an efficient way of training the AI model.

reflection: |
  Overall today taught me a lot about changing up the hyperparameters and seeing how well the model learned with different decay
  rates and learning rates. I made graphs to analyze the epsilon decay over each episode and the average rewards over vs the episodes.
  Since my model only gets penalized the rewards are always negative but the graph has sections with higher learning curves and more
  erratic ones. I hope to get to get to a place where the graph reflects a steady constant close to zero reward (the highest it can get).
---
