---
layout: post
title: "Day 17 - Training and Dueling DQN"
date: 2025-06-18
author: Anuva Nuzhat
permalink: /day17.html
tags: ["DQN", "RL", "Dueling DQN", "Training"]

what_i_learned: |
  This morning, we practiced pitching our models without using technical keywords so we could explain them to everyday people. After practicing, we presented our pitch to the high school teacher we were scheduled to meet today. These teachers are here to learn more about AI and hopefully pass on that knowledge to their students by observing our work and learning from our research.

After that, I began training my DQN (Deep Q-Network) model and experimented with different epsilon decay rates to see how they would impact the model’s learning. Unfortunately, this took quite a long time, as each change in the epsilon decay required the model to run again, which is time-consuming.

At first, I used a slower epsilon decay rate, and I noticed that after 1000 episodes, the epsilon value hadn’t decreased much. I then adjusted the decay rate to be faster so that epsilon would decrease more significantly during training. The goal is for the agent to reduce its reliance on random actions over time and instead use what it has learned to make better decisions. However, it's important that epsilon doesn't reach zero too early—keeping a small amount of exploration (usually with a minimum epsilon like 0.01) helps the agent avoid getting stuck in suboptimal behaviors.

After finishing training with this standard DQN model, my goal for tomorrow is to start working on a Dueling DQN. My teammates are exploring other variations of DQN, so we can compare them and identify which one performs best for our dataset.

blockers: |
  The models took quite a long time to run and when experimenting with different epsilon decay rates or learning rates it
  wasn't an efficient way of training the AI model.

reflection: |
  Overall today taught me a lot about changing up the hyperparameters and seeing how well the model learned with different decay
  rates and learning rates. I made graphs to analyze the epsilon decay over each episode and the average rewards over vs the episodes.
  Since my model only gets penalized the rewards are always negative but the graph has sections with higher learning curves and more
  erratic ones. I hope to get to get to a place where the graph reflects a steady constant close to zero reward (the highest it can get).
---
