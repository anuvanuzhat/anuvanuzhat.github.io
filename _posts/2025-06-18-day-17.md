---
layout: post
title: "Day 17 - Training and Dueling DQN"
date: 2025-06-18
author: Anuva Nuzhat
permalink: /day17.html
tags: ["DQN", "RL", "Dueling DQN", "Training"]

what_i_learned: |
  This morning, we practiced pitching our models without using technical keywords so we could explain them to everyday people. 
  After practicing, we presented our pitch to the high school teacher we were scheduled to meet today. These teachers are here 
  to learn more about AI and hopefully pass that knowledge on to their students by observing our work and learning from our research.

  After that, I began training my DQN (Deep Q-Network) model and experimented with different epsilon decay rates to see how they 
  would impact the model’s learning. Unfortunately, this took quite a long time, as each change in the epsilon decay required 
  the model to run again, which is time-consuming.

  At first, I used a slower epsilon decay rate, and I noticed that after 1000 episodes, the epsilon value hadn’t decreased much. 
  I then adjusted the decay rate to be faster so that epsilon would decrease more significantly during training. The goal is for 
  the agent to reduce its reliance on random actions over time and instead use what it has learned to make better decisions. 
  However, it's important that epsilon doesn’t reach zero too early—keeping a small amount of exploration (usually with a 
  minimum epsilon like 0.01) helps the agent avoid getting stuck in suboptimal behaviors.

  After finishing training with this standard DQN model, my goal for tomorrow is to start working on a Dueling DQN. My teammates 
  are exploring other variations of DQN, so we can compare them and identify which one performs best for our dataset.

blockers: |
  The models took quite a long time to run, and experimenting with different epsilon decay rates or learning rates wasn't an 
  efficient process. Each change required retraining the model from scratch, which slowed progress.

reflection: |
  Overall, today taught me a lot about tweaking hyperparameters and observing how they influence learning performance. 
  I created graphs to analyze epsilon decay across episodes and tracked average rewards over time. Since my model only gets 
  penalized, the rewards are always negative, but the graph shows sections with clear learning progress and others that are 
  more erratic. I hope to eventually reach a point where the graph reflects a steady trend toward zero reward (which is the 
  highest achievable in this case).
---
