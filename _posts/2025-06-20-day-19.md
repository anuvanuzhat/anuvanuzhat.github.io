---
layout: post
title: "Day 19 - Dueling DQN Implementation + Sucessful Training"
date: 2025-06-20
author: Anuva Nuzhat
permalink: /day19.html
tags: ["DQN", "RL", "Dueling DQN", "Training"]

what_i_learned: |
  Today I began implementing my Dueling DQN adding onto my previous DQN algorithm implementation. I was suprised to find that I didn't need to add 
  that many new lines of code and that the change was simple but effective. Instead of calculating the q values directly in a Dueling DQN it splits
  the q value into two components. These components are the value of being in the state V(s) and the advantage of the specific action in the state A(s, a1).
  The q value is then calculated by V + A - mean(A). So after adding a couple lines to account for the value stream and the advantage stream I began to train
  my model. The model at first was not doing any better than the normal DQN, it was actually doing worse. I decided to add some rewards to my model
  instead of just penalizing the model. After this I noticed a huge improvement for my model. I also noticed that the Dueling DQN was much faster
  than the normal DQN which is a huge added benefit.

blockers: |
  No Blockers.

reflection: |
  Working on the two models was very insightful to see how the DQN's differed. So far with no formal research it seems my dueling DQN is performing better
  but I think I need to play around with the epsilon decay rates and the learning rates to how well I can optimize the model. I'm excited to see
  how they fare against the other DQN models my lab mates were working on. My predicition is that the Noisy or Dueling DQN will perform the best but
  that is for next week. Since my lab mate seems to be having trouble implementing the Rainbow DQN I may work on it over this weekend or the next to 
  see how the Rainbow differs as well. 
---
