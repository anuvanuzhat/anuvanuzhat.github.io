---
layout: post
title: "Day 16 - Writing the RL DQN Model"
date: 2025-06-17
author: Anuva Nuzhat
permalink: /day16.html
tags: ["DQN", "RL"]

what_i_learned: |
  Today we began working on building our DQN models. The plan is to start with a basic DQN and later compare it with a Noisy DQN 
  to see how they perform differently. It took me the whole day to finish implementing the basic DQN. Tomorrow, I hope to train it 
  and generate some graphs to analyze the results.

  Since I had zero experience with DQN models, this was my first real introduction to how they work. I started by making edits 
  to my T1DHypertension environment, which currently uses randomly generated physiological parameters instead of the actual dataset. 
  Then, I moved on to developing my agent and training loops. I learned how to calculate Q-values using the DQN formula and got familiar 
  with the basics of PyTorch while building my model. I defined my hyperparameters, created the experience replay loop, and finalized 
  the basic DQN setup. I still need to experiment with the optimize function to improve the model’s learning efficiency.

blockers: |
  No blockers.

reflection: |
  Honestly, I learned so much today that my brain kind of hurts—but it's days like these that are the most important for learning! 
  Over the rest of the week, I hope to solidify what I’ve picked up so far. I'm really proud of the code I wrote today and super excited 
  to see how it performs once training starts.

  I haven’t started on the Noisy DQN yet, so I’m a bit intimidated, but I’m also looking forward to diving into it. I'm also thinking 
  about trying out a Dueling DQN and comparing its performance with the other models. After weeks of preparation and research, 
  it feels amazing to finally begin the implementation!
---
