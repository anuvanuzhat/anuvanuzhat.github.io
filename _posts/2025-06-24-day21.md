---
layout: post
title: "Day 21 â€“ Starting Over - Calculating New Rewards "
date: 2025-06-24
author: Anuva Nuzhat
permalink: /day21.html
tags: ["TD1", "RL", "Training", "DQN", "Dueling DQN"]

what_i_learned: |
  Today I spent the entire day trying to optimize my model. I made an accuracy function to see how many times over the course of a different
  range of episodes my model was able to successfully be in the correct blood pressure and glucose ranges. Originally my accuracy was around 19%, so not great.
  Then I got it up to 50% so much better but that was short lived. 
  After some tweaks it went down to 0% and no matter what changes I made it stayed at that. I think my model has learned optimal blood pressure
  ranges but it's really struggling with the most optimal glucose range which is why I keep failing at each episode and getting a 0% accuracy.
   
blockers: |
  Figuring out the best reward and penalty combos to get my model to learn is extremely difficult.

reflection: |
   Failing so many times meant that I needed to change something bigger so I went ahead and changed the number of discrete choices my model could make from 9 to 25.
   Through this change I noticed that for some of the episodes my glucose levels were finally hitting target. My new accuracy percent went up to 21.5% from 0% which is a
   huge improvement! My epsilon decay graph also decreased at a much better rate as well after I had changed it from .9995 to .995 and slightly increased 
   my learning rate. I think for my next steps I could review my penality points and increase my number of episodes. 
