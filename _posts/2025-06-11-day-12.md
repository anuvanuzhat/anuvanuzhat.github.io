---
layout: post
title: "Day 12 - RL Framework and Team Bonding"
date: 2025-06-11
author: Anuva Nuzhat
permalink: /day12.html
tags: ["Reinforcement Learning"]

what_i_learned: |
  Today I looked over some RL code framework for T1D and Hypertension, which we're leaning towards focusing on for our comorbidity project. First, we set up the physiological parameters, initialized the patient state, discretized continuous physiological values, executed medical actions, and then created a dual-objective reward function. We also have a separate class for the comorbidity Q-agent where we initialized the Q-table with tuple keys, chose an epsilon-greedy action, and updated the Q-learning rule. I also reviewed a training loop for our agent.

blockers: |
  No Blockers.

reflection: |
  Reviewing the RL framework for managing T1D and Hypertension comorbidity today was an important step in transitioning from understanding individual disease models to thinking about the complexity of coexisting conditions. We also moved towards abstraction by creating separate classes for the comorbidity Q-agent, which helped organize the logic and learning mechanics more clearly. Looking ahead, Iâ€™m curious to explore how our discretization choices affect learning efficiency and how the training loop might be adapted for more complex agent behavior.
---
