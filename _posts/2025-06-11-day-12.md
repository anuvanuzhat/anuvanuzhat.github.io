---
layout: post
title: "Day 12 - RL Framework and Team Bonding"
date: 2025-06-11
author: Anuva Nuzhat
permalink: /day12.html
tags: ["Reinforcement Learning"]

what_i_learned: |
   Today I looked over some RL code framework for T1D and Hypertension which we're leaning towards focusing on for our comorbidity. First 
   we set up the physiological parameters, initialized patient state, discretize continuous physiological values, execute medical
   action and then create a dual-object reward function. We also have a seperate class for the comorbidity q agent where we initialized the 
   q table with keys, chose an epsilon greedy action, and updated the q learning rule. I also looked over a training loop for our agent. We 
   then finished the day with a team bonding exercise.

blockers: |
  No Blockers.

reflection: |
    Reviewing the RL framework for managing T1D and Hypertension comorbidity today was an important step in transitioning from understanding 
    individual disease models to looking at the complexity of coexisting conditions. We also moved towards abstraction today by creating 
    separate classes for the comorbidity Q-agent. Further I hope to look at how the discretization choices impact learning efficiency or how
    the training loop could be adapted for more complex agents.
---
